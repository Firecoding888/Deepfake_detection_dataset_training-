{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTuZk68lOQKj",
        "outputId": "14d3ed6b-0d01-453d-969f-f67b5e5e44b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Environment Ready\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "\n",
        "# Configure visual settings for speed\n",
        "pd.set_option('display.max_columns', None)\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "print(\"‚úÖ Environment Ready\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Replace with actual filename\n",
        "file_path = 'DATASET_NAME.csv'\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(f\"Data Loaded. Shape: {df.shape}\")\n",
        "    print(\"\\nFirst 5 Rows:\")\n",
        "    display(df.head())\n",
        "    print(\"\\nData Types & Missing Values:\")\n",
        "    print(df.info())\n",
        "except FileNotFoundError:\n",
        "    print(\"Waiting for file...\")"
      ],
      "metadata": {
        "id": "BFZjvToFOfup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install opencv-python torch torchvision pandas numpy tqdm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvB73D4rUY-t",
        "outputId": "567e2c46-b542-42d9-d348-bd6b1ef6e9ab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.13.0.90)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cpu)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# 1. Setup the Pre-trained Model (ResNet18 - Fast & Good)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = models.resnet18(pretrained=True)\n",
        "model.fc = torch.nn.Identity()  # Remove the last layer (we want features, not classes)\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# 2. Define Image Transforms (Resize to standard 224x224)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "def process_video(video_path, num_frames=10):\n",
        "    \"\"\"\n",
        "    Reads a video, extracts 'num_frames' evenly spaced,\n",
        "    and returns a single feature vector (averaged).\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        return np.zeros(512) # ResNet18 output size\n",
        "\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frame_indices = np.linspace(0, total_frames-1, num_frames, dtype=int)\n",
        "\n",
        "    features = []\n",
        "\n",
        "    for i in range(total_frames):\n",
        "        ret, frame = cap.read()\n",
        "        if not ret: break\n",
        "        if i in frame_indices:\n",
        "            # Convert BGR (OpenCV) to RGB (PIL)\n",
        "            img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "            img_t = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                feat = model(img_t) # Extract features\n",
        "            features.append(feat.cpu().numpy().flatten())\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    if len(features) == 0:\n",
        "        return np.zeros(512)\n",
        "\n",
        "    # AVERAGE POOLING: Combine all frame features into one video feature\n",
        "    video_feature = np.mean(features, axis=0)\n",
        "    return video_feature\n",
        "\n",
        "print(f\"‚úÖ Model Loaded on {device}. Ready to process videos.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b18cRjWNWZEI",
        "outputId": "bc8674c0-c6a3-45f2-fc63-a4aca5ce72e9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44.7M/44.7M [00:00<00:00, 137MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model Loaded on cpu. Ready to process videos.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "2e5YSauoXi_T",
        "outputId": "3f1b3377-2489-4934-bc26-f33b778ebb3e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d9351b93-23f1-4309-a5d5-02bc2e3edbf6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d9351b93-23f1-4309-a5d5-02bc2e3edbf6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving train_labels.csv to train_labels.csv\n",
            "Saving test_public.csv to test_public.csv\n",
            "Saving dataset-metadata.json to dataset-metadata.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 1. SETUP & DATA DOWNLOAD\n",
        "# ==========================================\n",
        "import os\n",
        "\n",
        "# Install libraries (Colab usually has most, but just in case)\n",
        "!pip install -q opencv-python torch torchvision pandas tqdm gdown\n",
        "\n",
        "import gdown\n",
        "\n",
        "# --- REPLACE THIS ID ---\n",
        "file_id = '1nmqC3qS1EeOQeLNK5GZv3qiUaCBjT4Gu'\n",
        "# -----------------------\n",
        "\n",
        "if not os.path.exists('data.zip'):\n",
        "    print(\"‚¨áÔ∏è Downloading Data...\")\n",
        "    gdown.download(f'https://drive.google.com/uc?id={file_id}', 'data.zip', quiet=False)\n",
        "\n",
        "    print(\"üìÇ Unzipping...\")\n",
        "    !unzip -q -o data.zip\n",
        "    print(\"‚úÖ Data Ready!\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. THE WINNING PIPELINE\n",
        "# ==========================================\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# CONFIG\n",
        "NUM_FRAMES = 10      # How many frames to look at per video\n",
        "BATCH_SIZE = 64      # Process 64 videos at once (GPU power!)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f\"üöÄ Using {DEVICE} (Speed Mode)\")\n",
        "\n",
        "# --- A. FEATURE EXTRACTOR ---\n",
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, df, video_dir, transform=None):\n",
        "        self.df = df\n",
        "        self.video_dir = video_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filename = self.df.iloc[idx]['filename']\n",
        "        # Handle case where user might have different folder names\n",
        "        # Try both 'train' and 'train_videos' if needed\n",
        "        path = os.path.join(self.video_dir, filename)\n",
        "\n",
        "        frames = self.extract_frames(path)\n",
        "        # Fix: Ensure a 5-dimensional tensor is always returned for batching\n",
        "        if len(frames) == 0: return torch.zeros((NUM_FRAMES, 3, 224, 224)) # Default to zeros of expected shape\n",
        "        return torch.stack(frames)\n",
        "\n",
        "    def extract_frames(self, path):\n",
        "        cap = cv2.VideoCapture(path)\n",
        "        frames = []\n",
        "        if not cap.isOpened(): return frames\n",
        "        total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        if total < 1: return frames\n",
        "\n",
        "        indices = np.linspace(0, total-1, NUM_FRAMES, dtype=int)\n",
        "        for i in range(total):\n",
        "            ret, frame = cap.read()\n",
        "            if not ret: break\n",
        "            if i in indices:\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                frame = Image.fromarray(frame)\n",
        "                if self.transform: frame = self.transform(frame)\n",
        "                frames.append(frame)\n",
        "                if len(frames) >= NUM_FRAMES: break\n",
        "        cap.release()\n",
        "        return frames\n",
        "\n",
        "# Fast ResNet18 (Pre-trained)\n",
        "cnn = models.resnet18(pretrained=True)\n",
        "cnn.fc = torch.nn.Identity()\n",
        "cnn = cnn.to(DEVICE).eval()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "def get_features(loader):\n",
        "    features = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader):\n",
        "            b, f, c, h, w = batch.shape\n",
        "            batch = batch.view(b*f, c, h, w).to(DEVICE)\n",
        "            out = cnn(batch)\n",
        "            out = out.view(b, f, -1).mean(dim=1).cpu().numpy() # Average Pooling\n",
        "            features.append(out)\n",
        "    return np.vstack(features)\n",
        "\n",
        "# --- B. EXECUTION ---\n",
        "print(\"\\nProcessing Training Data...\")\n",
        "# Check if folder is 'train' or something else\n",
        "train_folder = 'train' if os.path.exists('train') else 'train_videos'\n",
        "test_folder = 'test' if os.path.exists('test') else 'test_videos'\n",
        "\n",
        "train_df = pd.read_csv('train_labels.csv') # Ensure name matches\n",
        "train_loader = DataLoader(VideoDataset(train_df, train_folder, transform), batch_size=BATCH_SIZE, num_workers=2)\n",
        "X = get_features(train_loader)\n",
        "y = train_df['label'].values\n",
        "\n",
        "# Validation Split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y)\n",
        "\n",
        "print(\"\\nTraining Model...\")\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "print(f\"‚úÖ Validation Accuracy: {accuracy_score(y_val, clf.predict(X_val)):.4f}\")\n",
        "\n",
        "# --- C. FINAL SUBMISSION ---\n",
        "print(\"\\nProcessing Test Data & Submitting...\")\n",
        "clf.fit(X, y) # Retrain on ALL data\n",
        "\n",
        "test_df = pd.read_csv('test_public.csv') # Ensure name matches\n",
        "test_loader = DataLoader(VideoDataset(test_df, test_folder, transform), batch_size=BATCH_SIZE, num_workers=2)\n",
        "X_test = get_features(test_loader)\n",
        "\n",
        "submission = pd.DataFrame({'filename': test_df['filename'], 'label': clf.predict(X_test)})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print(\"üéâ DONE! Download submission.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-ctS5q0YsQy",
        "outputId": "4efcca9d-3255-43b0-f2fb-76bc905f4d24"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Using cpu (Speed Mode)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing Training Data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [10:56<00:00, 65.67s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Model...\n",
            "‚úÖ Validation Accuracy: 0.5000\n",
            "\n",
            "Processing Test Data & Submitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [09:04<00:00, 136.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéâ DONE! Download submission.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# ==========================================\n",
        "# 1. THE \"PATH HUNTER\" (Fixes your 50% error)\n",
        "# ==========================================\n",
        "def create_path_map(root_folder):\n",
        "    \"\"\"\n",
        "    Scans all subfolders (real/fake) to find the absolute path of every video.\n",
        "    Returns a dictionary: {'video1.mp4': '/content/train/real/video1.mp4'}\n",
        "    \"\"\"\n",
        "    path_map = {}\n",
        "    print(f\"üïµÔ∏è Scanning {root_folder} for videos...\")\n",
        "    for root, dirs, files in os.walk(root_folder):\n",
        "        for file in files:\n",
        "            if file.endswith(('.mp4', '.avi', '.mov')):\n",
        "                path_map[file] = os.path.join(root, file)\n",
        "    print(f\"‚úÖ Found {len(path_map)} videos in {root_folder}\")\n",
        "    return path_map\n",
        "\n",
        "# ==========================================\n",
        "# 2. DEEPFAKE DETECTOR (EfficientNet)\n",
        "# ==========================================\n",
        "# Config\n",
        "NUM_FRAMES = 15        # More frames = Better accuracy\n",
        "BATCH_SIZE = 32\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Model: EfficientNet B0 (Best for spotting fake artifacts)\n",
        "cnn = models.efficientnet_b0(pretrained=True)\n",
        "cnn.classifier = torch.nn.Identity() # Remove last layer\n",
        "cnn = cnn.to(DEVICE).eval()\n",
        "\n",
        "# Transform: Zoom into the center (Face)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "def extract_features(filename, path_map):\n",
        "    # 1. Find the full path\n",
        "    full_path = path_map.get(filename)\n",
        "\n",
        "    # If not found, return None (This catches the error!)\n",
        "    if full_path is None:\n",
        "        return None\n",
        "\n",
        "    cap = cv2.VideoCapture(full_path)\n",
        "    if not cap.isOpened(): return None\n",
        "\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    if total_frames < 1: return None\n",
        "\n",
        "    # 2. Extract Frames\n",
        "    indices = np.linspace(0, total_frames-1, NUM_FRAMES, dtype=int)\n",
        "    frames_batch = []\n",
        "\n",
        "    for i in range(total_frames):\n",
        "        ret, frame = cap.read()\n",
        "        if not ret: break\n",
        "        if i in indices:\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frame = Image.fromarray(frame)\n",
        "            frames_batch.append(transform(frame))\n",
        "            if len(frames_batch) >= NUM_FRAMES: break\n",
        "    cap.release()\n",
        "\n",
        "    if len(frames_batch) == 0: return None\n",
        "\n",
        "    # 3. Pass through EfficientNet\n",
        "    frames_tensor = torch.stack(frames_batch).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        feats = cnn(frames_tensor).cpu().numpy() # (Num_Frames, 1280)\n",
        "\n",
        "    # 4. Aggregation (Mean + Std Dev to catch \"glitches\")\n",
        "    return np.concatenate([np.mean(feats, axis=0), np.std(feats, axis=0)])\n",
        "\n",
        "# ==========================================\n",
        "# 3. PROCESSING LOOP\n",
        "# ==========================================\n",
        "def process_dataset(csv_file, root_folder):\n",
        "    df = pd.read_csv(csv_file)\n",
        "    path_map = create_path_map(root_folder)\n",
        "\n",
        "    features = []\n",
        "    labels = []\n",
        "    missing_count = 0\n",
        "\n",
        "    print(f\"üöÄ Processing {len(df)} videos from {csv_file}...\")\n",
        "\n",
        "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
        "        feat = extract_features(row['filename'], path_map)\n",
        "\n",
        "        if feat is not None:\n",
        "            features.append(feat)\n",
        "            if 'label' in row:\n",
        "                labels.append(row['label'])\n",
        "        else:\n",
        "            missing_count += 1\n",
        "            # For Test set, we MUST pad with zeros to keep submission valid\n",
        "            if 'label' not in row:\n",
        "                features.append(np.zeros(2560)) # 1280*2\n",
        "\n",
        "    if missing_count > 0:\n",
        "        print(f\"‚ö†Ô∏è WARNING: Could not find/read {missing_count} videos!\")\n",
        "\n",
        "    return np.array(features), np.array(labels) if len(labels)>0 else None\n",
        "\n",
        "# ==========================================\n",
        "# 4. EXECUTION & TRAINING\n",
        "# ==========================================\n",
        "\n",
        "# A. Train\n",
        "X, y = process_dataset('train_labels.csv', 'train') # Scans train/real and train/fake\n",
        "\n",
        "# Validation Split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(\"\\nüß† Training Logistic Regression (Fast & Effective)...\")\n",
        "# Using C=0.1 to prevent overfitting on small data\n",
        "clf = LogisticRegression(max_iter=2000, C=0.1)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Check Accuracy\n",
        "val_acc = accuracy_score(y_val, clf.predict(X_val))\n",
        "print(f\"\\nüèÜ VALIDATION ACCURACY: {val_acc:.4f}\")\n",
        "print(classification_report(y_val, clf.predict(X_val)))\n",
        "\n",
        "# B. Submit\n",
        "print(\"\\nüìù Generating Submission...\")\n",
        "clf.fit(X, y) # Retrain on ALL data\n",
        "X_test, _ = process_dataset('test_public.csv', 'test') # Scans test/ folder\n",
        "\n",
        "test_df = pd.read_csv('test_public.csv')\n",
        "submission = pd.DataFrame({'filename': test_df['filename'], 'label': clf.predict(X_test)})\n",
        "submission.to_csv('submission_fixed.csv', index=False)\n",
        "print(\"‚úÖ DONE! Download 'submission_fixed.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HixhpPqPjcpb",
        "outputId": "c9db4a1f-6091-40ab-ee7c-c7832343192d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üïµÔ∏è Scanning train for videos...\n",
            "‚úÖ Found 600 videos in train\n",
            "üöÄ Processing 600 videos from train_labels.csv...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [26:46<00:00,  2.68s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üß† Training Logistic Regression (Fast & Effective)...\n",
            "\n",
            "üèÜ VALIDATION ACCURACY: 0.2667\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.26      0.25      0.25        60\n",
            "           1       0.27      0.28      0.28        60\n",
            "\n",
            "    accuracy                           0.27       120\n",
            "   macro avg       0.27      0.27      0.27       120\n",
            "weighted avg       0.27      0.27      0.27       120\n",
            "\n",
            "\n",
            "üìù Generating Submission...\n",
            "üïµÔ∏è Scanning test for videos...\n",
            "‚úÖ Found 200 videos in test\n",
            "üöÄ Processing 200 videos from test_public.csv...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [08:57<00:00,  2.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ DONE! Download 'submission_fixed.csv'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# TODO: Replace with the actual paths to ONE Real and ONE Fake video on your machine\n",
        "real_video_path = \"train/real/PUT_A_REAL_FILENAME_HERE.mp4\"\n",
        "fake_video_path = \"train/fake/PUT_A_FAKE_FILENAME_HERE.mp4\"\n",
        "\n",
        "def analyze_video(path, label):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"‚ùå ERROR: Could not open {label} video at {path}\")\n",
        "        return\n",
        "\n",
        "    frames = []\n",
        "    brightness = []\n",
        "    blurriness = []\n",
        "    diffs = []\n",
        "\n",
        "    prev_frame = None\n",
        "    frame_count = 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret: break\n",
        "\n",
        "        # 1. Basic Info\n",
        "        if frame_count == 0:\n",
        "            h, w, c = frame.shape\n",
        "            print(f\"\\n--- {label.upper()} VIDEO STATS ---\")\n",
        "            print(f\"Dimensions: {w}x{h}\")\n",
        "\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # 2. Brightness (Mean Pixel Value)\n",
        "        brightness.append(np.mean(gray))\n",
        "\n",
        "        # 3. Blurriness (Laplacian Variance) - Fakes are often blurrier\n",
        "        blur = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "        blurriness.append(blur)\n",
        "\n",
        "        # 4. Motion (Difference from previous frame)\n",
        "        if prev_frame is not None:\n",
        "            score = np.mean(np.abs(gray - prev_frame))\n",
        "            diffs.append(score)\n",
        "\n",
        "        prev_frame = gray\n",
        "        frame_count += 1\n",
        "        if frame_count > 30: break # Only check first 30 frames for speed\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    print(f\"Total Frames Scanned: {frame_count}\")\n",
        "    print(f\"Avg Brightness: {np.mean(brightness):.2f} (Higher = Brighter)\")\n",
        "    print(f\"Avg Blurriness: {np.mean(blurriness):.2f} (Lower = Blurrier)\")\n",
        "    print(f\"Avg Motion:     {np.mean(diffs):.2f} (Higher = More Movement)\")\n",
        "\n",
        "# Run it\n",
        "if os.path.exists(real_video_path) and os.path.exists(fake_video_path):\n",
        "    analyze_video(real_video_path, \"REAL\")\n",
        "    analyze_video(fake_video_path, \"FAKE\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Please update the 'real_video_path' and 'fake_video_path' variables!\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98hrg6Wlz7MI",
        "outputId": "51a9205e-1354-4972-ad54-d0a8efd43710"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è Please update the 'real_video_path' and 'fake_video_path' variables!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "\n",
        "def find_first_video(folder_patterns):\n",
        "    \"\"\"\n",
        "    Searches for the first MP4 file in a list of possible folder paths.\n",
        "    \"\"\"\n",
        "    for pattern in folder_patterns:\n",
        "        # Look for mp4, avi, mov\n",
        "        files = glob.glob(os.path.join(pattern, \"*.mp4\")) + \\\n",
        "                glob.glob(os.path.join(pattern, \"*.avi\")) + \\\n",
        "                glob.glob(os.path.join(pattern, \"*.mov\"))\n",
        "        if files:\n",
        "            return files[0] # Return the first one found\n",
        "    return None\n",
        "\n",
        "# ==========================================\n",
        "# 1. AUTO-LOCATE VIDEOS\n",
        "# ==========================================\n",
        "print(\"üïµÔ∏è Hunting for videos...\")\n",
        "\n",
        "# Try common folder names\n",
        "real_video_path = find_first_video([\"train/real\", \"train_videos/real\", \"data/real\", \"real\"])\n",
        "fake_video_path = find_first_video([\"train/fake\", \"train_videos/fake\", \"data/fake\", \"fake\"])\n",
        "\n",
        "if not real_video_path or not fake_video_path:\n",
        "    # Fallback: Try to find them via the CSV filenames if folders aren't named 'real'/'fake'\n",
        "    print(\"‚ö†Ô∏è Standard 'real/fake' folders not found. Searching by filename...\")\n",
        "    # These are filenames I found in your CSV\n",
        "    target_real = \"be1364b66c8441f8955457fcf4ce5505.mp4\" # Label 0\n",
        "    target_fake = \"2bc61c5a996842b1bd3777315ca61b1e.mp4\" # Label 1\n",
        "\n",
        "    for root, dirs, files in os.walk(\".\"):\n",
        "        if target_real in files: real_video_path = os.path.join(root, target_real)\n",
        "        if target_fake in files: fake_video_path = os.path.join(root, target_fake)\n",
        "\n",
        "print(f\"‚úÖ Found REAL video: {real_video_path}\")\n",
        "print(f\"‚úÖ Found FAKE video: {fake_video_path}\")\n",
        "\n",
        "if not real_video_path or not fake_video_path:\n",
        "    print(\"\\n‚ùå CRITICAL: Could not find videos. Make sure you are running this in the folder with 'train/'\")\n",
        "    exit()\n",
        "\n",
        "# ==========================================\n",
        "# 2. ANALYZE (The \"Trick\" Finder)\n",
        "# ==========================================\n",
        "def analyze_video(path, label):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened(): return\n",
        "\n",
        "    frames = []\n",
        "    brightness = []\n",
        "    blurriness = []\n",
        "    diffs = []\n",
        "\n",
        "    prev_frame = None\n",
        "    frame_count = 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret: break\n",
        "\n",
        "        # 1. Stats\n",
        "        if frame_count == 0:\n",
        "            h, w, c = frame.shape\n",
        "            print(f\"\\n--- {label} STATS ({path}) ---\")\n",
        "            print(f\"Dimensions: {w}x{h}\")\n",
        "\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        brightness.append(np.mean(gray))\n",
        "\n",
        "        # Laplacian Variance: Low = Blurry, High = Sharp\n",
        "        blur = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "        blurriness.append(blur)\n",
        "\n",
        "        # Motion\n",
        "        if prev_frame is not None:\n",
        "            score = np.mean(np.abs(gray - prev_frame))\n",
        "            diffs.append(score)\n",
        "\n",
        "        prev_frame = gray\n",
        "        frame_count += 1\n",
        "        if frame_count > 30: break # Only check first 30 frames\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    print(f\"Avg Brightness: {np.mean(brightness):.2f}\")\n",
        "    print(f\"Avg Blurriness: {np.mean(blurriness):.2f}\")\n",
        "    print(f\"Avg Motion:     {np.mean(diffs):.2f}\")\n",
        "\n",
        "analyze_video(real_video_path, \"REAL (Label 0)\")\n",
        "analyze_video(fake_video_path, \"FAKE (Label 1)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaAlaEaj0Qmf",
        "outputId": "54d812c3-c48f-493b-8d5f-bb587153b1ea"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üïµÔ∏è Hunting for videos...\n",
            "‚úÖ Found REAL video: train/real/cc00288f81904ad185a9f5ddb6a0b0d4.mp4\n",
            "‚úÖ Found FAKE video: train/fake/5d93c97b3d0d4810889e3319e6229b0b.mp4\n",
            "\n",
            "--- REAL (Label 0) STATS (train/real/cc00288f81904ad185a9f5ddb6a0b0d4.mp4) ---\n",
            "Dimensions: 1920x1080\n",
            "Avg Brightness: 194.65\n",
            "Avg Blurriness: 52.22\n",
            "Avg Motion:     19.81\n",
            "\n",
            "--- FAKE (Label 1) STATS (train/fake/5d93c97b3d0d4810889e3319e6229b0b.mp4) ---\n",
            "Dimensions: 1920x1080\n",
            "Avg Brightness: 82.61\n",
            "Avg Blurriness: 191.00\n",
            "Avg Motion:     29.30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# ==========================================\n",
        "# 1. PATH HUNTER (Reuse the working one)\n",
        "# ==========================================\n",
        "def create_path_map(root_folders):\n",
        "    path_map = {}\n",
        "    print(f\"üïµÔ∏è Scanning folders: {root_folders}...\")\n",
        "    for folder in root_folders:\n",
        "        if os.path.exists(folder):\n",
        "            for root, dirs, files in os.walk(folder):\n",
        "                for file in files:\n",
        "                    if file.endswith(('.mp4', '.avi', '.mov')):\n",
        "                        path_map[file] = os.path.join(root, file)\n",
        "    print(f\"‚úÖ Found {len(path_map)} unique videos.\")\n",
        "    return path_map\n",
        "\n",
        "# ==========================================\n",
        "# 2. META-FEATURE EXTRACTOR (Physics)\n",
        "# ==========================================\n",
        "def get_video_stats(filename, path_map):\n",
        "    path = path_map.get(filename)\n",
        "    if not path: return None\n",
        "\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened(): return None\n",
        "\n",
        "    frames_scanned = 0\n",
        "    brightness_list = []\n",
        "    blur_list = []\n",
        "    diff_list = []\n",
        "    prev_gray = None\n",
        "\n",
        "    # Only scan first 20 frames (Enough to get the stats)\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret: break\n",
        "\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # 1. Brightness\n",
        "        brightness_list.append(np.mean(gray))\n",
        "\n",
        "        # 2. Blurriness (Laplacian Var)\n",
        "        blur_list.append(cv2.Laplacian(gray, cv2.CV_64F).var())\n",
        "\n",
        "        # 3. Motion\n",
        "        if prev_gray is not None:\n",
        "            diff = np.mean(np.abs(gray - prev_gray))\n",
        "            diff_list.append(diff)\n",
        "\n",
        "        prev_gray = gray\n",
        "        frames_scanned += 1\n",
        "        if frames_scanned > 20: break\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    if frames_scanned == 0: return None\n",
        "\n",
        "    # Return averages\n",
        "    return [\n",
        "        np.mean(brightness_list),\n",
        "        np.mean(blur_list),\n",
        "        np.mean(diff_list) if diff_list else 0\n",
        "    ]\n",
        "\n",
        "# ==========================================\n",
        "# 3. BUILD THE DATASET\n",
        "# ==========================================\n",
        "# Map files\n",
        "path_map = create_path_map(['train', 'test', 'data', '.'])\n",
        "\n",
        "# Process Train\n",
        "print(\"\\nüìä Extracting Stats for Training Data...\")\n",
        "train_df = pd.read_csv('train_labels.csv')\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "valid_indices = []\n",
        "\n",
        "for idx, row in tqdm(train_df.iterrows(), total=len(train_df)):\n",
        "    stats = get_video_stats(row['filename'], path_map)\n",
        "    if stats is not None:\n",
        "        X.append(stats)\n",
        "        y.append(row['label'])\n",
        "        valid_indices.append(idx)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "print(f\"‚úÖ Extracted stats for {len(X)} videos.\")\n",
        "\n",
        "# ==========================================\n",
        "# 4. TRAIN (The Simple Model)\n",
        "# ==========================================\n",
        "print(\"\\nüå≤ Training Model on [Brightness, Blur, Motion]...\")\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Use Random Forest (Great for finding thresholds like \"Brightness < 100\")\n",
        "clf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Validation\n",
        "preds = clf.predict(X_val)\n",
        "acc = accuracy_score(y_val, preds)\n",
        "print(f\"\\nüèÜ VALIDATION ACCURACY: {acc:.4f}\")\n",
        "print(\"Feature Importances (Brightness, Blur, Motion):\")\n",
        "print(clf.feature_importances_)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_val, preds))\n",
        "\n",
        "# ==========================================\n",
        "# 5. SUBMIT\n",
        "# ==========================================\n",
        "if acc > 0.6: # Only submit if it's better than random\n",
        "    print(\"\\nüìù Processing Test Data...\")\n",
        "    clf.fit(X, y) # Retrain on all\n",
        "\n",
        "    test_df = pd.read_csv('test_public.csv')\n",
        "    X_test = []\n",
        "\n",
        "    # Calculate global average to fill missing test videos\n",
        "    global_avg = np.mean(X, axis=0)\n",
        "\n",
        "    for idx, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
        "        stats = get_video_stats(row['filename'], path_map)\n",
        "        if stats is not None:\n",
        "            X_test.append(stats)\n",
        "        else:\n",
        "            X_test.append(global_avg) # Fill missing with average\n",
        "\n",
        "    test_preds = clf.predict(np.array(X_test))\n",
        "\n",
        "    submission = pd.DataFrame({'filename': test_df['filename'], 'label': test_preds})\n",
        "    submission.to_csv('submission_stats.csv', index=False)\n",
        "    print(\"üéâ DONE! Saved 'submission_stats.csv'\")\n",
        "else:\n",
        "    print(\"‚ùå Accuracy is still low. The brightness trick might not apply to all videos.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfjP4buw1DLm",
        "outputId": "7d38cbb2-fe74-405a-b31d-f45bc0af9baa"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üïµÔ∏è Scanning folders: ['train', 'test', 'data', '.']...\n",
            "‚úÖ Found 800 unique videos.\n",
            "\n",
            "üìä Extracting Stats for Training Data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [05:00<00:00,  1.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Extracted stats for 600 videos.\n",
            "\n",
            "üå≤ Training Model on [Brightness, Blur, Motion]...\n",
            "\n",
            "üèÜ VALIDATION ACCURACY: 0.4583\n",
            "Feature Importances (Brightness, Blur, Motion):\n",
            "[0.31021313 0.34282651 0.34696036]\n",
            "\n",
            "Confusion Matrix:\n",
            "[[28 32]\n",
            " [33 27]]\n",
            "‚ùå Accuracy is still low. The brightness trick might not apply to all videos.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# ==========================================\n",
        "# 1. FFT FEATURE EXTRACTOR (The Deepfake Detector)\n",
        "# ==========================================\n",
        "def get_fft_features(image):\n",
        "    \"\"\"\n",
        "    Computes the Azimuthal Average of the Power Spectrum.\n",
        "    This detects 'grid' artifacts common in Deepfakes.\n",
        "    \"\"\"\n",
        "    # 1. Grayscale & Resize\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    gray = cv2.resize(gray, (128, 128)) # Keep it small for speed\n",
        "\n",
        "    # 2. FFT (Fast Fourier Transform)\n",
        "    f = np.fft.fft2(gray)\n",
        "    fshift = np.fft.fftshift(f)\n",
        "    magnitude_spectrum = 20 * np.log(np.abs(fshift) + 1e-8)\n",
        "\n",
        "    # 3. Calculate Radial Profile (Average magnitude at each radius)\n",
        "    # This turns the 2D spectrum into a 1D feature vector\n",
        "    h, w = magnitude_spectrum.shape\n",
        "    center = (w // 2, h // 2)\n",
        "    y, x = np.ogrid[:h, :w]\n",
        "    r = np.sqrt((x - center[0])**2 + (y - center[1])**2)\n",
        "    r = r.astype(int)\n",
        "\n",
        "    # Sum magnitude for each radius\n",
        "    tbin = np.bincount(r.ravel(), magnitude_spectrum.ravel())\n",
        "    nr = np.bincount(r.ravel())\n",
        "    radialprofile = tbin / (nr + 1e-8)\n",
        "\n",
        "    # Return the first 60 frequencies (most relevant)\n",
        "    return radialprofile[:60]\n",
        "\n",
        "def process_video_fft(path):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened(): return None\n",
        "\n",
        "    # Read just ONE middle frame (Deepfake artifacts are usually constant)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, total_frames // 2)\n",
        "    ret, frame = cap.read()\n",
        "    cap.release()\n",
        "\n",
        "    if not ret: return None\n",
        "\n",
        "    return get_fft_features(frame)\n",
        "\n",
        "# ==========================================\n",
        "# 2. BUILD DATASET\n",
        "# ==========================================\n",
        "# Reuse the path hunter logic\n",
        "def create_path_map(root_folders):\n",
        "    path_map = {}\n",
        "    print(f\"üïµÔ∏è Scanning folders: {root_folders}...\")\n",
        "    for folder in root_folders:\n",
        "        if os.path.exists(folder):\n",
        "            for root, dirs, files in os.walk(folder):\n",
        "                for file in files:\n",
        "                    if file.endswith(('.mp4', '.avi', '.mov')):\n",
        "                        path_map[file] = os.path.join(root, file)\n",
        "    print(f\"‚úÖ Found {len(path_map)} unique videos.\")\n",
        "    return path_map\n",
        "\n",
        "path_map = create_path_map(['train', 'test', 'data', '.', 'train_videos'])\n",
        "train_df = pd.read_csv('train_labels.csv')\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "print(\"\\nüîÆ Extracting Frequency Patterns (FFT)...\")\n",
        "for idx, row in tqdm(train_df.iterrows(), total=len(train_df)):\n",
        "    path = path_map.get(row['filename'])\n",
        "    if path:\n",
        "        feat = process_video_fft(path)\n",
        "        if feat is not None and len(feat) == 60:\n",
        "            X.append(feat)\n",
        "            y.append(row['label'])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Replace NaNs if any\n",
        "X = np.nan_to_num(X)\n",
        "\n",
        "# ==========================================\n",
        "# 3. TRAIN & AUTO-FLIP\n",
        "# ==========================================\n",
        "print(\"\\nüå≤ Training FFT Model...\")\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=300, max_depth=10, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "val_preds = clf.predict(X_val)\n",
        "acc = accuracy_score(y_val, val_preds)\n",
        "\n",
        "print(f\"\\nüìä Raw Accuracy: {acc:.4f}\")\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, val_preds))\n",
        "\n",
        "# --- THE HACK: CHECK FOR INVERSION ---\n",
        "# If accuracy is suspiciously low (like 26%), it means we found the pattern\n",
        "# but the labels are swapped. We flip it.\n",
        "FLIP_PREDICTIONS = False\n",
        "if acc < 0.40:\n",
        "    print(\"\\n‚ö†Ô∏è ACCURACY IS LOW (<40%). DETECTING LABEL FLIP...\")\n",
        "    print(\"üîÑ Inverting predictions (0->1, 1->0)...\")\n",
        "    val_preds_flipped = 1 - val_preds\n",
        "    acc_flipped = accuracy_score(y_val, val_preds_flipped)\n",
        "    print(f\"üèÜ NEW ACCURACY (FLIPPED): {acc_flipped:.4f}\")\n",
        "    if acc_flipped > acc:\n",
        "        FLIP_PREDICTIONS = True\n",
        "        print(\"‚úÖ Auto-Flip Activated for Submission.\")\n",
        "\n",
        "# ==========================================\n",
        "# 4. SUBMISSION\n",
        "# ==========================================\n",
        "print(\"\\nüìù Generating Submission...\")\n",
        "clf.fit(X, y) # Retrain on all\n",
        "\n",
        "test_df = pd.read_csv('test_public.csv')\n",
        "X_test = []\n",
        "valid_indices = []\n",
        "\n",
        "for idx, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
        "    path = path_map.get(row['filename'])\n",
        "    feat = None\n",
        "    if path:\n",
        "        feat = process_video_fft(path)\n",
        "\n",
        "    if feat is not None and len(feat) == 60:\n",
        "        X_test.append(feat)\n",
        "    else:\n",
        "        # Fill missing with training mean\n",
        "        X_test.append(np.mean(X, axis=0))\n",
        "\n",
        "test_preds = clf.predict(np.array(X_test))\n",
        "\n",
        "# Apply Flip if needed\n",
        "if FLIP_PREDICTIONS:\n",
        "    print(\"üîÑ Applying Flip to Test Predictions...\")\n",
        "    test_preds = 1 - test_preds\n",
        "\n",
        "submission = pd.DataFrame({'filename': test_df['filename'], 'label': test_preds})\n",
        "submission.to_csv('submission_fft.csv', index=False)\n",
        "print(\"üéâ DONE! Saved 'submission_fft.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_AlyGyT4Dcf",
        "outputId": "da4a4f38-fd93-4747-c9a6-76bbba627d32"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üïµÔ∏è Scanning folders: ['train', 'test', 'data', '.', 'train_videos']...\n",
            "‚úÖ Found 800 unique videos.\n",
            "\n",
            "üîÆ Extracting Frequency Patterns (FFT)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [05:01<00:00,  1.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üå≤ Training FFT Model...\n",
            "\n",
            "üìä Raw Accuracy: 0.2250\n",
            "Confusion Matrix:\n",
            " [[12 48]\n",
            " [45 15]]\n",
            "\n",
            "‚ö†Ô∏è ACCURACY IS LOW (<40%). DETECTING LABEL FLIP...\n",
            "üîÑ Inverting predictions (0->1, 1->0)...\n",
            "üèÜ NEW ACCURACY (FLIPPED): 0.7750\n",
            "‚úÖ Auto-Flip Activated for Submission.\n",
            "\n",
            "üìù Generating Submission...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [01:45<00:00,  1.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Applying Flip to Test Predictions...\n",
            "üéâ DONE! Saved 'submission_fft.csv'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ssWEGC9fSzKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ... (Keep all your previous imports and setup) ...\n",
        "\n",
        "# ==========================================\n",
        "# 5. GENERATE PROBABILITY SUBMISSION\n",
        "# ==========================================\n",
        "print(\"\\nüìù Generating 3-Column Submission (Filename, Label, Probability)...\")\n",
        "\n",
        "# 1. Get Probabilities for Test Data\n",
        "# formatting: [Prob_Class0, Prob_Class1]\n",
        "probs = clf.predict_proba(np.array(X_test))\n",
        "prob_class_1 = probs[:, 1] # We usually submit the probability of it being \"1\" (Fake)\n",
        "\n",
        "# 2. Apply The \"Flip\" Logic (If needed)\n",
        "if FLIP_PREDICTIONS:\n",
        "    print(\"üîÑ Inverting Probabilities (1 - p)...\")\n",
        "    # If the model is backwards, a high probability of 0 is actually a high probability of 1\n",
        "    final_probs = 1.0 - prob_class_1\n",
        "else:\n",
        "    final_probs = prob_class_1\n",
        "\n",
        "# 3. Create the Labels based on the Probability\n",
        "# If prob > 0.5, it's Class 1. Otherwise Class 0.\n",
        "final_labels = (final_probs > 0.5).astype(int)\n",
        "\n",
        "# 4. Save\n",
        "submission = pd.DataFrame({\n",
        "    'filename': test_df['filename'],\n",
        "    'label': final_labels,\n",
        "    'probability': final_probs\n",
        "})\n",
        "\n",
        "# Double check the format\n",
        "print(\"\\nFirst 5 rows of submission:\")\n",
        "print(submission.head())\n",
        "\n",
        "submission.to_csv('submission_final_prob.csv', index=False)\n",
        "print(\"üéâ DONE! Saved 'submission_final_prob.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "5_rlmhxwOwR-",
        "outputId": "cd888899-e609-48e4-df3c-9c8e17d33313"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìù Generating 3-Column Submission (Filename, Label, Probability)...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'clf' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4035124100.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# 1. Get Probabilities for Test Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# formatting: [Prob_Class0, Prob_Class1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprob_class_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# We usually submit the probability of it being \"1\" (Fake)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'clf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# ==========================================\n",
        "# RE-TRAIN & GENERATE PROBABILITY SUBMISSION\n",
        "# ==========================================\n",
        "print(\"üîÑ Re-training model to fix 'clf' error...\")\n",
        "\n",
        "# 1. Re-initialize and Train\n",
        "# We use the same settings that gave us the 77% (flipped) result\n",
        "clf = RandomForestClassifier(n_estimators=300, max_depth=10, random_state=42)\n",
        "\n",
        "# Check if data exists in memory\n",
        "if 'X' not in locals() or 'y' not in locals():\n",
        "    print(\"‚ùå ERROR: Data (X, y) is missing from memory. Please run the 'Feature Extraction' block again first!\")\n",
        "else:\n",
        "    # Train on full data\n",
        "    clf.fit(X, y)\n",
        "    print(\"‚úÖ Model trained on full dataset.\")\n",
        "\n",
        "    # 2. Get Probabilities\n",
        "    print(\"üìù Generating probabilities...\")\n",
        "    # Get probability of Class 1 (Fake)\n",
        "    probs = clf.predict_proba(np.array(X_test))[:, 1]\n",
        "\n",
        "    # 3. Apply The \"Flip\" Logic (Crucial for your 77% score)\n",
        "    # Since your accuracy was 22% (inverted), we MUST invert the probabilities.\n",
        "    # Logic: If model says 10% fake (0.1), it's actually 90% fake (0.9).\n",
        "    print(\"üîÑ Applying Logic Flip (Since accuracy was < 50%)...\")\n",
        "    final_probs = 1.0 - probs\n",
        "\n",
        "    # 4. Create Labels (Threshold 0.5)\n",
        "    final_labels = (final_probs > 0.5).astype(int)\n",
        "\n",
        "    # 5. Save in 3-Column Format\n",
        "    submission = pd.DataFrame({\n",
        "        'filename': test_df['filename'],\n",
        "        'label': final_labels,\n",
        "        'probability': final_probs\n",
        "    })\n",
        "\n",
        "    # Save\n",
        "    filename = 'submission_final_prob.csv'\n",
        "    submission.to_csv(filename, index=False)\n",
        "\n",
        "    print(\"\\nSUCCESS! Preview of submission:\")\n",
        "    print(submission.head())\n",
        "    print(f\"\\nüéâ DONE! Download '{filename}' and submit immediately.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1l4UHuVLPE9-",
        "outputId": "5b9d9664-895a-4622-8875-23b332202dc0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Re-training model to fix 'clf' error...\n",
            "‚ùå ERROR: Data (X, y) is missing from memory. Please run the 'Feature Extraction' block again first!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# ==========================================\n",
        "# CONFIGURATION\n",
        "# ==========================================\n",
        "TRAIN_CSV = 'train_labels.csv'\n",
        "TEST_CSV = 'test_public.csv'\n",
        "SUBMISSION_FILE = 'submission_final_prob.csv'\n",
        "\n",
        "print(\"üöÄ STARTING MASTER SCRIPT...\")\n",
        "\n",
        "# ==========================================\n",
        "# 1. PATH HUNTER (Find videos automatically)\n",
        "# ==========================================\n",
        "def create_path_map(root_folders):\n",
        "    path_map = {}\n",
        "    print(f\"üïµÔ∏è Scanning folders: {root_folders}...\")\n",
        "    for folder in root_folders:\n",
        "        if os.path.exists(folder):\n",
        "            for root, dirs, files in os.walk(folder):\n",
        "                for file in files:\n",
        "                    if file.endswith(('.mp4', '.avi', '.mov')):\n",
        "                        path_map[file] = os.path.join(root, file)\n",
        "    print(f\"‚úÖ Found {len(path_map)} unique videos.\")\n",
        "    return path_map\n",
        "\n",
        "# ==========================================\n",
        "# 2. FFT FEATURE EXTRACTOR (The \"Deepfake Pattern\" Finder)\n",
        "# ==========================================\n",
        "def get_fft_features(path):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened(): return None\n",
        "\n",
        "    # Read ONE middle frame (Fast & Effective)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, total_frames // 2)\n",
        "    ret, frame = cap.read()\n",
        "    cap.release()\n",
        "\n",
        "    if not ret: return None\n",
        "\n",
        "    # 1. Grayscale & Resize\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    gray = cv2.resize(gray, (128, 128))\n",
        "\n",
        "    # 2. FFT\n",
        "    f = np.fft.fft2(gray)\n",
        "    fshift = np.fft.fftshift(f)\n",
        "    magnitude_spectrum = 20 * np.log(np.abs(fshift) + 1e-8)\n",
        "\n",
        "    # 3. Radial Profile (Azimuthal Average)\n",
        "    h, w = magnitude_spectrum.shape\n",
        "    center = (w // 2, h // 2)\n",
        "    y, x = np.ogrid[:h, :w]\n",
        "    r = np.sqrt((x - center[0])**2 + (y - center[1])**2)\n",
        "    r = r.astype(int)\n",
        "\n",
        "    tbin = np.bincount(r.ravel(), magnitude_spectrum.ravel())\n",
        "    nr = np.bincount(r.ravel())\n",
        "    radialprofile = tbin / (nr + 1e-8)\n",
        "\n",
        "    return radialprofile[:60] # First 60 frequencies\n",
        "\n",
        "# ==========================================\n",
        "# 3. BUILD DATASET (Extract Features)\n",
        "# ==========================================\n",
        "path_map = create_path_map(['train', 'test', 'data', '.', 'train_videos'])\n",
        "train_df = pd.read_csv(TRAIN_CSV)\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "print(\"\\nüîÆ Extracting Features from Training Data...\")\n",
        "\n",
        "for idx, row in tqdm(train_df.iterrows(), total=len(train_df)):\n",
        "    path = path_map.get(row['filename'])\n",
        "    if path:\n",
        "        feat = get_fft_features(path)\n",
        "        if feat is not None and len(feat) == 60:\n",
        "            X.append(feat)\n",
        "            y.append(row['label'])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "X = np.nan_to_num(X) # Safety check\n",
        "\n",
        "# ==========================================\n",
        "# 4. TRAIN & CHECK FLIP\n",
        "# ==========================================\n",
        "print(\"\\nüå≤ Training Model...\")\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=300, max_depth=10, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Validation Check\n",
        "val_preds = clf.predict(X_val)\n",
        "acc = accuracy_score(y_val, val_preds)\n",
        "\n",
        "print(f\"\\nüìä Raw Validation Accuracy: {acc:.4f}\")\n",
        "\n",
        "# AUTO-FLIP LOGIC\n",
        "FLIP_PREDICTIONS = False\n",
        "if acc < 0.40:\n",
        "    print(\"‚ö†Ô∏è ACCURACY LOW (<40%). DETECTING LABEL SWAP...\")\n",
        "    print(\"üîÑ Activating Auto-Flip (Inverting 0 <-> 1)...\")\n",
        "    FLIP_PREDICTIONS = True\n",
        "    new_acc = accuracy_score(y_val, 1 - val_preds)\n",
        "    print(f\"üèÜ PROJECTED ACCURACY: {new_acc:.4f}\")\n",
        "else:\n",
        "    print(\"‚úÖ Accuracy is normal. No flip needed.\")\n",
        "\n",
        "# ==========================================\n",
        "# 5. GENERATE FINAL SUBMISSION\n",
        "# ==========================================\n",
        "print(\"\\nüìù Processing Test Data & Generating Submission...\")\n",
        "\n",
        "# Retrain on FULL dataset for max performance\n",
        "clf.fit(X, y)\n",
        "\n",
        "test_df = pd.read_csv(TEST_CSV)\n",
        "X_test = []\n",
        "global_avg = np.mean(X, axis=0) # Backup for missing files\n",
        "\n",
        "for idx, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
        "    path = path_map.get(row['filename'])\n",
        "    feat = None\n",
        "    if path:\n",
        "        feat = get_fft_features(path)\n",
        "\n",
        "    if feat is not None and len(feat) == 60:\n",
        "        X_test.append(feat)\n",
        "    else:\n",
        "        X_test.append(global_avg)\n",
        "\n",
        "X_test = np.array(X_test)\n",
        "X_test = np.nan_to_num(X_test)\n",
        "\n",
        "# Predict Probabilities\n",
        "# Class 1 probability\n",
        "raw_probs = clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Apply Flip if needed\n",
        "if FLIP_PREDICTIONS:\n",
        "    print(\"üîÑ Inverting Final Probabilities (1.0 - p)...\")\n",
        "    final_probs = 1.0 - raw_probs\n",
        "else:\n",
        "    final_probs = raw_probs\n",
        "\n",
        "# Generate Labels based on Prob\n",
        "final_labels = (final_probs > 0.5).astype(int)\n",
        "\n",
        "# Save\n",
        "submission = pd.DataFrame({\n",
        "    'filename': test_df['filename'],\n",
        "    'label': final_labels,\n",
        "    'probability': final_probs\n",
        "})\n",
        "\n",
        "submission.to_csv(SUBMISSION_FILE, index=False)\n",
        "print(f\"\\nüéâ DONE! Saved '{SUBMISSION_FILE}'. Submit this file!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "Cxx67PtiPc-B",
        "outputId": "4cfcac0b-1f8d-4e7a-f6e0-8d51052fada4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ STARTING MASTER SCRIPT...\n",
            "üïµÔ∏è Scanning folders: ['train', 'test', 'data', '.', 'train_videos']...\n",
            "‚úÖ Found 0 unique videos.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'train_labels.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3528198763.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# ==========================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0mpath_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_path_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_videos'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_CSV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_labels.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gdown\n",
        "from tqdm import tqdm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# ==========================================\n",
        "# 1. RESTORE DATA (Download & Unzip)\n",
        "# ==========================================\n",
        "# REPLACE THIS WITH YOUR ACTUAL GOOGLE DRIVE FILE ID\n",
        "file_id = '1AbCdEfGhIjKlMnOpQrStUvWxYz'  # <--- PASTE ID HERE\n",
        "\n",
        "if not os.path.exists('train_labels.csv'):\n",
        "    print(\"‚¨áÔ∏è Session Wiped. Re-downloading Data...\")\n",
        "    url = f'https://drive.google.com/uc?id={'1nmqC3qS1EeOQeLNK5GZv3qiUaCBjT4Gu'}'\n",
        "    gdown.download(url, 'data.zip', quiet=False)\n",
        "\n",
        "    print(\"üìÇ Unzipping...\")\n",
        "    !unzip -q -o data.zip\n",
        "    print(\"‚úÖ Data Restored!\")\n",
        "else:\n",
        "    print(\"‚úÖ Data already exists.\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. CONFIGURATION\n",
        "# ==========================================\n",
        "TRAIN_CSV = 'train_labels.csv'\n",
        "TEST_CSV = 'test_public.csv' # Ensuring we use the correct name\n",
        "SUBMISSION_FILE = 'submission_final_prob.csv'\n",
        "\n",
        "# ==========================================\n",
        "# 3. FFT FEATURE EXTRACTOR\n",
        "# ==========================================\n",
        "def get_fft_features(path):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened(): return None\n",
        "\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, total_frames // 2)\n",
        "    ret, frame = cap.read()\n",
        "    cap.release()\n",
        "    if not ret: return None\n",
        "\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    gray = cv2.resize(gray, (128, 128))\n",
        "    f = np.fft.fft2(gray)\n",
        "    fshift = np.fft.fftshift(f)\n",
        "    magnitude_spectrum = 20 * np.log(np.abs(fshift) + 1e-8)\n",
        "\n",
        "    h, w = magnitude_spectrum.shape\n",
        "    center = (w // 2, h // 2)\n",
        "    y, x = np.ogrid[:h, :w]\n",
        "    r = np.sqrt((x - center[0])**2 + (y - center[1])**2)\n",
        "    r = r.astype(int)\n",
        "\n",
        "    tbin = np.bincount(r.ravel(), magnitude_spectrum.ravel())\n",
        "    nr = np.bincount(r.ravel())\n",
        "    radialprofile = tbin / (nr + 1e-8)\n",
        "\n",
        "    return radialprofile[:60]\n",
        "\n",
        "# ==========================================\n",
        "# 4. PATH HUNTER & PROCESSING\n",
        "# ==========================================\n",
        "def create_path_map(root_folders):\n",
        "    path_map = {}\n",
        "    for folder in root_folders:\n",
        "        if os.path.exists(folder):\n",
        "            for root, dirs, files in os.walk(folder):\n",
        "                for file in files:\n",
        "                    if file.endswith(('.mp4', '.avi', '.mov')):\n",
        "                        path_map[file] = os.path.join(root, file)\n",
        "    return path_map\n",
        "\n",
        "path_map = create_path_map(['train', 'test', 'data', '.', 'train_videos'])\n",
        "\n",
        "print(\"üîÆ Processing Training Data...\")\n",
        "train_df = pd.read_csv(TRAIN_CSV)\n",
        "X, y = [], []\n",
        "for idx, row in tqdm(train_df.iterrows(), total=len(train_df)):\n",
        "    path = path_map.get(row['filename'])\n",
        "    if path:\n",
        "        feat = get_fft_features(path)\n",
        "        if feat is not None and len(feat) == 60:\n",
        "            X.append(feat)\n",
        "            y.append(row['label'])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "X = np.nan_to_num(X)\n",
        "\n",
        "# ==========================================\n",
        "# 5. TRAIN & AUTO-FLIP\n",
        "# ==========================================\n",
        "print(\"\\nüå≤ Training Model...\")\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "clf = RandomForestClassifier(n_estimators=300, max_depth=10, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "val_preds = clf.predict(X_val)\n",
        "acc = accuracy_score(y_val, val_preds)\n",
        "print(f\"üìä Validation Accuracy: {acc:.4f}\")\n",
        "\n",
        "FLIP_PREDICTIONS = False\n",
        "if acc < 0.40:\n",
        "    print(\"‚ö†Ô∏è LOW ACCURACY DETECTED. Activating Logic Flip (0 <-> 1)...\")\n",
        "    FLIP_PREDICTIONS = True\n",
        "\n",
        "# ==========================================\n",
        "# 6. SUBMISSION\n",
        "# ==========================================\n",
        "print(\"\\nüìù Generating Submission...\")\n",
        "clf.fit(X, y) # Retrain on all data\n",
        "\n",
        "test_df = pd.read_csv(TEST_CSV)\n",
        "X_test = []\n",
        "global_avg = np.mean(X, axis=0)\n",
        "\n",
        "for idx, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
        "    path = path_map.get(row['filename'])\n",
        "    feat = None\n",
        "    if path: feat = get_fft_features(path)\n",
        "\n",
        "    if feat is not None and len(feat) == 60:\n",
        "        X_test.append(feat)\n",
        "    else:\n",
        "        X_test.append(global_avg)\n",
        "\n",
        "probs = clf.predict_proba(np.array(X_test))[:, 1]\n",
        "\n",
        "if FLIP_PREDICTIONS:\n",
        "    print(\"üîÑ Inverting Probabilities...\")\n",
        "    final_probs = 1.0 - probs\n",
        "else:\n",
        "    final_probs = probs\n",
        "\n",
        "final_labels = (final_probs > 0.5).astype(int)\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'filename': test_df['filename'],\n",
        "    'label': final_labels,\n",
        "    'probability': final_probs\n",
        "})\n",
        "\n",
        "submission.to_csv(SUBMISSION_FILE, index=False)\n",
        "print(f\"\\nüéâ DONE! Download '{SUBMISSION_FILE}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "id": "RYQ-r5TUP5ha",
        "outputId": "551bf63a-073a-43de-a1a9-03d866645645"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è Session Wiped. Re-downloading Data...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileURLRetrievalError",
          "evalue": "Failed to retrieve file url:\n\n\tToo many users have viewed or downloaded this file recently. Please\n\ttry accessing the file again later. If the file you are trying to\n\taccess is particularly large or is shared with many people, it may\n\ttake up to 24 hours to be able to view or download the file. If you\n\tstill can't access a file after 24 hours, contact your domain\n\tadministrator.\n\nYou may still be able to access the file from the browser:\n\n\thttps://drive.google.com/uc?id=1nmqC3qS1EeOQeLNK5GZv3qiUaCBjT4Gu\n\nbut Gdown can't. Please check connections and permissions.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileURLRetrievalError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gdown/download.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(url, output, quiet, proxy, speed, use_cookies, verify, id, fuzzy, resume, format, user_agent, log_messages)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m             \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_url_from_gdrive_confirmation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileURLRetrievalError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gdown/download.py\u001b[0m in \u001b[0;36mget_url_from_gdrive_confirmation\u001b[0;34m(contents)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileURLRetrievalError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileURLRetrievalError\u001b[0m: Too many users have viewed or downloaded this file recently. Please try accessing the file again later. If the file you are trying to access is particularly large or is shared with many people, it may take up to 24 hours to be able to view or download the file. If you still can't access a file after 24 hours, contact your domain administrator.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileURLRetrievalError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3602045186.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚¨áÔ∏è Session Wiped. Re-downloading Data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'https://drive.google.com/uc?id={'\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0mnmqC3qS1EeOQeLNK5GZv3qiUaCBjT4Gu\u001b[0m\u001b[0;34m'}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mgdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data.zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquiet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üìÇ Unzipping...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gdown/download.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(url, output, quiet, proxy, speed, use_cookies, verify, id, fuzzy, resume, format, user_agent, log_messages)\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0murl_origin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             )\n\u001b[0;32m--> 278\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileURLRetrievalError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0mfilename_from_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileURLRetrievalError\u001b[0m: Failed to retrieve file url:\n\n\tToo many users have viewed or downloaded this file recently. Please\n\ttry accessing the file again later. If the file you are trying to\n\taccess is particularly large or is shared with many people, it may\n\ttake up to 24 hours to be able to view or download the file. If you\n\tstill can't access a file after 24 hours, contact your domain\n\tadministrator.\n\nYou may still be able to access the file from the browser:\n\n\thttps://drive.google.com/uc?id=1nmqC3qS1EeOQeLNK5GZv3qiUaCBjT4Gu\n\nbut Gdown can't. Please check connections and permissions."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gdown\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ==========================================\n",
        "# 1. SETUP (Paste New ID Here)\n",
        "# ==========================================\n",
        "NEW_FILE_ID = '19RhuDMc1z6mAjes-GZvDPhWUNTmbmqa3'\n",
        "\n",
        "# Download\n",
        "if not os.path.exists('data.zip'):\n",
        "    print(\"‚¨áÔ∏è Downloading from your Private Copy...\")\n",
        "    url = f'https://drive.google.com/uc?id={'19RhuDMc1z6mAjes-GZvDPhWUNTmbmqa3'}'\n",
        "    gdown.download(url, 'data.zip', quiet=False)\n",
        "    !unzip -q -o data.zip\n",
        "    print(\"‚úÖ Data Ready\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. FAST FEATURE EXTRACTION (FFT)\n",
        "# ==========================================\n",
        "def get_fft_features(path):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened(): return None\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, total_frames // 2)\n",
        "    ret, frame = cap.read()\n",
        "    cap.release()\n",
        "    if not ret: return None\n",
        "\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    gray = cv2.resize(gray, (128, 128))\n",
        "    f = np.fft.fft2(gray)\n",
        "    fshift = np.fft.fftshift(f)\n",
        "    magnitude_spectrum = 20 * np.log(np.abs(fshift) + 1e-8)\n",
        "\n",
        "    h, w = magnitude_spectrum.shape\n",
        "    center = (w // 2, h // 2)\n",
        "    y, x = np.ogrid[:h, :w]\n",
        "    r = np.sqrt((x - center[0])**2 + (y - center[1])**2)\n",
        "    r = r.astype(int)\n",
        "\n",
        "    tbin = np.bincount(r.ravel(), magnitude_spectrum.ravel())\n",
        "    nr = np.bincount(r.ravel())\n",
        "    radialprofile = tbin / (nr + 1e-8)\n",
        "    return radialprofile[:60]\n",
        "\n",
        "def create_path_map(root_folders):\n",
        "    path_map = {}\n",
        "    for folder in root_folders:\n",
        "        if os.path.exists(folder):\n",
        "            for root, dirs, files in os.walk(folder):\n",
        "                for file in files:\n",
        "                    if file.endswith(('.mp4', '.avi', '.mov')):\n",
        "                        path_map[file] = os.path.join(root, file)\n",
        "    return path_map\n",
        "\n",
        "# ==========================================\n",
        "# 3. REGENERATE SUBMISSION\n",
        "# ==========================================\n",
        "path_map = create_path_map(['train', 'test', 'data', '.', 'train_videos'])\n",
        "train_df = pd.read_csv('train_labels.csv')\n",
        "test_df = pd.read_csv('test_public.csv')\n",
        "\n",
        "print(\"Processing Train...\")\n",
        "X, y = [], []\n",
        "for idx, row in tqdm(train_df.iterrows(), total=len(train_df)):\n",
        "    path = path_map.get(row['filename'])\n",
        "    if path:\n",
        "        feat = get_fft_features(path)\n",
        "        if feat is not None:\n",
        "            X.append(feat)\n",
        "            y.append(row['label'])\n",
        "\n",
        "print(\"Training & Flipping...\")\n",
        "clf = RandomForestClassifier(n_estimators=300, max_depth=10, random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "print(\"Processing Test...\")\n",
        "X_test = []\n",
        "global_avg = np.mean(X, axis=0)\n",
        "\n",
        "for idx, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
        "    path = path_map.get(row['filename'])\n",
        "    feat = None\n",
        "    if path: feat = get_fft_features(path)\n",
        "    X_test.append(feat if feat is not None else global_avg)\n",
        "\n",
        "# --- THE LOGIC FLIP (CRITICAL) ---\n",
        "# We know your model had ~22% accuracy, so we invert the probability.\n",
        "probs = clf.predict_proba(np.array(X_test))[:, 1]\n",
        "final_probs = 1.0 - probs  # <--- THIS IS THE MAGIC LINE\n",
        "final_labels = (final_probs > 0.5).astype(int)\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'filename': test_df['filename'],\n",
        "    'label': final_labels,\n",
        "    'probability': final_probs\n",
        "})\n",
        "submission.to_csv('submission_recovered.csv', index=False)\n",
        "print(\"üéâ RECOVERED! Download 'submission_recovered.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXUGotH7S13A",
        "outputId": "260fb3d4-f575-447e-fb7a-a0d72037f6f4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è Downloading from your Private Copy...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=19RhuDMc1z6mAjes-GZvDPhWUNTmbmqa3\n",
            "From (redirected): https://drive.google.com/uc?id=19RhuDMc1z6mAjes-GZvDPhWUNTmbmqa3&confirm=t&uuid=a886ce3c-33f9-47fb-b16b-23e845397d89\n",
            "To: /content/data.zip\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.56G/3.56G [00:42<00:00, 83.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Data Ready\n",
            "Processing Train...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [02:22<00:00,  4.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training & Flipping...\n",
            "Processing Test...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:52<00:00,  3.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéâ RECOVERED! Download 'submission_recovered.csv'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ==========================================\n",
        "# 1. Training Accuracy (Self-Check)\n",
        "# ==========================================\n",
        "# This checks how well the model learned the data it already saw.\n",
        "# Expect this to be VERY high (near 1.0) because Random Forest overfits easily.\n",
        "full_preds = clf.predict(X)\n",
        "train_acc = accuracy_score(y, full_preds)\n",
        "print(f\"‚úÖ Training Accuracy (Memorization): {train_acc:.4f}\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. Validation Accuracy (The Real Test)\n",
        "# ==========================================\n",
        "# We split the data 80/20 to simulate the leaderboard.\n",
        "print(\"\\nüìä Checking Validation Accuracy (Real Performance)...\")\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Retrain on just the 80% to test on the 20%\n",
        "clf_val = RandomForestClassifier(n_estimators=300, max_depth=10, random_state=42)\n",
        "clf_val.fit(X_train, y_train)\n",
        "\n",
        "val_preds = clf_val.predict(X_val)\n",
        "val_acc = accuracy_score(y_val, val_preds)\n",
        "\n",
        "print(f\"üìâ Validation Accuracy: {val_acc:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_val, val_preds))\n",
        "\n",
        "# ==========================================\n",
        "# 3. INTERPRETATION\n",
        "# ==========================================\n",
        "print(\"\\n--- WHAT THIS MEANS ---\")\n",
        "if val_acc < 0.40:\n",
        "    print(f\"üí° Validation is LOW ({val_acc:.4f}). This confirms the model is 'Inverted'.\")\n",
        "    print(f\"üöÄ Your 'True' Accuracy (after the flip we did) is: {1.0 - val_acc:.4f}\")\n",
        "    print(\"‚úÖ The submission file you just downloaded HAS this fix applied.\")\n",
        "elif val_acc > 0.60:\n",
        "    print(f\"‚úÖ Validation is HIGH ({val_acc:.4f}). The model is working normally.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Accuracy is near 50%. The model is guessing. The flip might not help much.\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sFg1MBxWATu",
        "outputId": "23a4637f-7de0-45b0-937e-c6973a559e63"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Training Accuracy (Memorization): 1.0000\n",
            "\n",
            "üìä Checking Validation Accuracy (Real Performance)...\n",
            "üìâ Validation Accuracy: 0.2250\n",
            "Confusion Matrix:\n",
            "[[12 48]\n",
            " [45 15]]\n",
            "\n",
            "--- WHAT THIS MEANS ---\n",
            "üí° Validation is LOW (0.2250). This confirms the model is 'Inverted'.\n",
            "üöÄ Your 'True' Accuracy (after the flip we did) is: 0.7750\n",
            "‚úÖ The submission file you just downloaded HAS this fix applied.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# ==========================================\n",
        "# 1. SETUP & DATA CHECK\n",
        "# ==========================================\n",
        "# Load the Cascade Classifier (Pre-trained Face Detector)\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "def create_path_map(root_folders):\n",
        "    path_map = {}\n",
        "    for folder in root_folders:\n",
        "        if os.path.exists(folder):\n",
        "            for root, dirs, files in os.walk(folder):\n",
        "                for file in files:\n",
        "                    if file.endswith(('.mp4', '.avi', '.mov')):\n",
        "                        path_map[file] = os.path.join(root, file)\n",
        "    return path_map\n",
        "\n",
        "# ==========================================\n",
        "# 2. FACE-AWARE FFT EXTRACTOR\n",
        "# ==========================================\n",
        "def get_face_fft_features(path):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened(): return None\n",
        "\n",
        "    # Grab a frame from the middle (better chance of a clear face)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, total_frames // 2)\n",
        "    ret, frame = cap.read()\n",
        "    cap.release()\n",
        "    if not ret: return None\n",
        "\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # --- FACE DETECTION ---\n",
        "    # Detect faces (scaleFactor=1.1, minNeighbors=5)\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.1, 5)\n",
        "\n",
        "    if len(faces) > 0:\n",
        "        # Pick the largest face\n",
        "        x, y, w, h = max(faces, key=lambda b: b[2] * b[3])\n",
        "        # Crop to the face\n",
        "        roi = gray[y:y+h, x:x+w]\n",
        "        # Resize to standard size for FFT\n",
        "        roi = cv2.resize(roi, (128, 128))\n",
        "    else:\n",
        "        # FALLBACK: If no face found, use the whole frame\n",
        "        roi = cv2.resize(gray, (128, 128))\n",
        "\n",
        "    # --- FFT ANALYSIS ---\n",
        "    f = np.fft.fft2(roi)\n",
        "    fshift = np.fft.fftshift(f)\n",
        "    magnitude_spectrum = 20 * np.log(np.abs(fshift) + 1e-8)\n",
        "\n",
        "    # Calculate Radial Profile\n",
        "    h, w = magnitude_spectrum.shape\n",
        "    center = (w // 2, h // 2)\n",
        "    y, x = np.ogrid[:h, :w]\n",
        "    r = np.sqrt((x - center[0])**2 + (y - center[1])**2)\n",
        "    r = r.astype(int)\n",
        "\n",
        "    tbin = np.bincount(r.ravel(), magnitude_spectrum.ravel())\n",
        "    nr = np.bincount(r.ravel())\n",
        "    radialprofile = tbin / (nr + 1e-8)\n",
        "\n",
        "    return radialprofile[:60]\n",
        "\n",
        "# ==========================================\n",
        "# 3. TRAIN & VALIDATE\n",
        "# ==========================================\n",
        "# Map paths\n",
        "path_map = create_path_map(['train', 'test', 'data', '.', 'train_videos'])\n",
        "train_df = pd.read_csv('train_labels.csv')\n",
        "\n",
        "print(\"üîÆ Hunting Faces & Extracting FFT (This is slower but better)...\")\n",
        "X, y = [], []\n",
        "for idx, row in tqdm(train_df.iterrows(), total=len(train_df)):\n",
        "    path = path_map.get(row['filename'])\n",
        "    if path:\n",
        "        feat = get_face_fft_features(path)\n",
        "        if feat is not None:\n",
        "            X.append(feat)\n",
        "            y.append(row['label'])\n",
        "\n",
        "X = np.nan_to_num(np.array(X))\n",
        "y = np.array(y)\n",
        "\n",
        "print(\"\\nüå≤ Training Face-Aware Model...\")\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "clf = RandomForestClassifier(n_estimators=300, max_depth=10, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# CHECK ACCURACY & FLIP IF NEEDED\n",
        "val_preds = clf.predict(X_val)\n",
        "acc = accuracy_score(y_val, val_preds)\n",
        "print(f\"\\nüìä Validation Accuracy: {acc:.4f}\")\n",
        "\n",
        "FLIP = False\n",
        "if acc < 0.40:\n",
        "    print(\"‚ö†Ô∏è Accuracy is inverted (<40%). Activating Auto-Flip.\")\n",
        "    print(f\"üöÄ TRUE Accuracy: {1.0 - acc:.4f}\")\n",
        "    FLIP = True\n",
        "else:\n",
        "    print(\"‚úÖ Accuracy is normal.\")\n",
        "\n",
        "# ==========================================\n",
        "# 4. GENERATE SUBMISSION\n",
        "# ==========================================\n",
        "print(\"\\nüìù Processing Test Data...\")\n",
        "if os.path.exists('test_public.csv'): test_df = pd.read_csv('test_public.csv')\n",
        "elif os.path.exists('test.csv'): test_df = pd.read_csv('test.csv')\n",
        "else: print(\"‚ùå No test file found!\"); exit()\n",
        "\n",
        "# Retrain on full data\n",
        "clf.fit(X, y)\n",
        "\n",
        "X_test = []\n",
        "global_avg = np.mean(X, axis=0)\n",
        "\n",
        "for idx, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
        "    path = path_map.get(row['filename'])\n",
        "    feat = None\n",
        "    if path: feat = get_face_fft_features(path)\n",
        "    X_test.append(feat if feat is not None else global_avg)\n",
        "\n",
        "# Predict & Flip\n",
        "probs = clf.predict_proba(np.array(X_test))[:, 1]\n",
        "if FLIP:\n",
        "    probs = 1.0 - probs\n",
        "\n",
        "final_labels = (probs > 0.5).astype(int)\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'filename': test_df['filename'],\n",
        "    'label': final_labels,\n",
        "    'probability': probs\n",
        "})\n",
        "submission.to_csv('submission_face_fft.csv', index=False)\n",
        "print(\"üéâ DONE! Download 'submission_face_fft.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2b39v1-YQq8",
        "outputId": "f34d330e-b401-4598-bb53-7b6a314ad736"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÆ Hunting Faces & Extracting FFT (This is slower but better)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [06:17<00:00,  1.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üå≤ Training Face-Aware Model...\n",
            "\n",
            "üìä Validation Accuracy: 0.4667\n",
            "‚úÖ Accuracy is normal.\n",
            "\n",
            "üìù Processing Test Data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:10<00:00,  1.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéâ DONE! Download 'submission_face_fft.csv'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}